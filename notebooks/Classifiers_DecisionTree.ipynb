{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Decision Tree\n",
    "\n",
    "#####Pros and Cons\n",
    "Why Decision Trees\n",
    " - Easily interpretable\n",
    " - Handles missing values and outliers\n",
    " - non-parametric/non-linear/model complex phenomenom\n",
    " - Computationally cheap to predict\n",
    " - Can handle irrelevant features\n",
    " - Mixed data (nominal and continuous)\n",
    " \n",
    "Why Not Decision Trees\n",
    " - Computationally expensive to train\n",
    " - Greedy algorithm (local optima)\n",
    " - Very easy to overfit\n",
    "\n",
    "#####PseudoCode\n",
    "\n",
    "- If every item in the dataset is in the same class or there is no feature left to split the data:\n",
    "    - return a leaf node with the class label\n",
    "- Else:\n",
    "    - find the best feature and value to split the data \n",
    "        - Methods\n",
    "            - Gini Impurity \n",
    "            - Information Gain\n",
    "    - split the dataset\n",
    "        - Categorical Variable: Choose either value or not (ex: sunny or not sunny)\n",
    "        - Continuous Variable: Threshold (ex: temp <75 or >= 75)\n",
    "    - create a node\n",
    "    - for each split\n",
    "        - call BuildTree and add the result as a child of the node\n",
    "    - return node\n",
    "\n",
    "#####Measure How to Split the Data\n",
    "Gini Impurity:\n",
    "Take a random element from the set\n",
    "Label it randomly according to the distribution of labels in the set\n",
    "What is the probability that it is labeled incorrectly?\n",
    "\n",
    "Information Gain: \n",
    "The entropy of a set is a measure of the amount of disorder. Intuitively, if a set has all the same labels, that'll have low entropy and if it has a mix of labels, that's high entropy. We would like to create splits that minimize the entropy in each size. If our splits do a good job splitting along the boundary between classes, they have more predictive power.\n",
    "We use information gain to determine the best split:\n",
    "\n",
    "#####Pruning\n",
    "As is mentioned above, Decision Trees are prone to overfitting. If we have a lot of features and they all get used in building our tree, we will build a tree that perfectly represents our training data but is not general. A way to relax this is pruning. The idea is that we may not want to continue building the tree until all the leaves are pure (have only datapoints of one class). There are two main ways of pruning: prepruning and postpruning.\n",
    "\n",
    "Prepruning is making the decision tree algorithm stop early. Here are a few ways that we preprune:\n",
    "\n",
    " - leaf size: \n",
    "    Stop when the number of data points for a leaf gets below a threshold\n",
    " - depth: \n",
    "    Stop when the depth of the tree (distance from root to leaf) reaches a threshold\n",
    " - mostly the same: Stop when some percent of the data points are the same (rather than all the same)\n",
    " - error threshold: Stop when the error reduction (information gain) isn't improved significantly.\n",
    "\n",
    "Postpruning involves building the tree first and then choosing to cut off some of the leaves \n",
    "```\n",
    "function Prune:\n",
    "    if either left or right is not a leaf:\n",
    "        call Prune on that split\n",
    "    if both left and right are leaf nodes:\n",
    "        calculate error associated with merging two nodes\n",
    "        calculate error associated without merging two nodes\n",
    "        if merging results in lower error:\n",
    "            merge the leaf nodes\n",
    "```\n",
    "####Regression Trees\n",
    "You can also use Decision Trees for regression! Instead of take a majority vote at each leaf node, if you're trying to predict a continuous value, you can average the values. You can also use a combination of decision trees and linear regression on the leaf nodes (called model trees)\n",
    "\n",
    "####Other Algorithms\n",
    "ID3\n",
    "\n",
    " - Short for Iterative Dichotomiser 3, the original Decision Tree algorithm developed by Ross Quinlan (who's responsible for a lot of proprietary decision tree algorithms) in the 1980's.\n",
    "\n",
    " - designed for only categorial features\n",
    "splits categorical features completely\n",
    "uses entropy and information gain to pick the best split\n",
    "\n",
    "CART\n",
    " - Short for Classification and Regression Tree was invented about the same time as ID3 by Breiman, Friedman, Olshen and Stone. The CART algorithm has the following properties:\n",
    "\n",
    " - handles both categorial and continuous data\n",
    "always uses binary splits\n",
    "uses gini impurity to pick the best split\n",
    "Algorithms will be called CART even if they don't follow all of the specifications of the original algorithm.\n",
    "\n",
    "C4.5\n",
    "\n",
    " -This is Quinlan's first improvement on the ID3 algorithm. The main improvements are:\n",
    "\n",
    " - handles continuous data\n",
    "implements pruning to reduce overfitting\n",
    "There is now a C5.0 which is supposedly better, but is propietary so we don't have access to the specifics of the improvements.\n",
    "\n",
    "####SKLEARN NOTES\n",
    "* Pruning with `max_depth`, `min_samples_split`, `min_samples_leaf` or `max_leaf_nodes`\n",
    "    - max_depth:\n",
    "        - controls depth of interaction\n",
    "        - so, how many branches the tree goes. May be a stump of 1. normally no more than 4, 6.\n",
    "    - min_samples_per_leaf\n",
    "        - don't want too few leaves (like only 1) , because then overfit to outliers\n",
    "* gini is default, but you can also choose entropy\n",
    "* does binary splits (you would need to binarize categorial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \n",
    "\n",
    "DTmodel = DecisionTreeClassifier(criterion = 'gini', max_depth =2, min_samples_leaf=1, max_features = None, max_leaf_node= None )\n",
    "Dtmodel.it(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
