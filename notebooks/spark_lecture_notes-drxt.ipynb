{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spark-logo.png\">\n",
    "\n",
    "Apache Spark\n",
    "============\n",
    "\n",
    "Spark Intro\n",
    "-----------\n",
    "\n",
    "What is Spark?\n",
    "\n",
    "- Spark is a framework for distributed processing.\n",
    "\n",
    "- It is a streamlined alternative to Map-Reduce.\n",
    "\n",
    "- Spark applications can be written in Python, Scala, or Java.\n",
    "\n",
    "Why Spark\n",
    "---------\n",
    "\n",
    "Why learn Spark?\n",
    "\n",
    "- Spark enables you to analyze petabytes of data.\n",
    "\n",
    "- Spark skills are in high demand--<http://indeed.com/salary>.\n",
    "\n",
    "- Spark is signficantly faster than MapReduce.\n",
    "\n",
    "- Paradoxically, Spark's API is simpler than the MapReduce API.\n",
    "\n",
    "Goals\n",
    "-----\n",
    "\n",
    "By the end of this lecture, you will be able to:\n",
    "\n",
    "- Create RDDs to distribute data across a cluster\n",
    "\n",
    "- Use the Spark shell to compose and execute Spark commands\n",
    "\n",
    "- Use Spark to analyze stock market data\n",
    "\n",
    "Spark Version History\n",
    "---------------------\n",
    "\n",
    "Date                Version         Changes\n",
    "----                -------         -------\n",
    "May 30, 2014        Spark 1.0.0     APIs stabilized \n",
    "September 11, 2014  Spark 1.1.0     New functions in MLlib, Spark SQL\n",
    "December 18, 2014   Spark 1.2.0     Python Streaming API and better streaming fault tolerance\n",
    "March 13, 2015      Spark 1.3.0     DataFrame API, Kafka integration in Streaming\n",
    "April 17, 2015      Spark 1.3.1     Bug fixes, minor changes\n",
    "\n",
    "Matei Zaharia\n",
    "-------------\n",
    "\n",
    "<img style=\"width:50%\" src=\"images/matei.jpg\">\n",
    "\n",
    "Essense of Spark\n",
    "----------------\n",
    "\n",
    "What is the basic idea of Spark?\n",
    "\n",
    "- Spark takes the Map-Reduce paradigm and changes it in some critical\n",
    "  ways.\n",
    "\n",
    "- Instead of writing single Map-Reduce jobs a Spark job consists of a\n",
    "  series of map and reduce functions. \n",
    "  \n",
    "- However, the intermediate data is kept in memory instead of being\n",
    "  written to disk or written to HDFS.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Since Spark keeps intermediate data in memory to get speed, what\n",
    "does it make us give up? Where's the catch?\n",
    "</summary>\n",
    "1. Spark does a trade-off between memory and performance.\n",
    "<br>\n",
    "2. While Spark apps are faster, they also consume more memory.\n",
    "<br>\n",
    "3. Spark outshines Map-Reduce in iterative algorithms where the\n",
    "   overhead of saving the results of each step to HDFS slows down\n",
    "   Map-Reduce.\n",
    "<br>\n",
    "4. For non-iterative algorithms Spark is comparable to Map-Reduce.\n",
    "</details>\n",
    "\n",
    "Spark Logging\n",
    "-------------\n",
    "\n",
    "Q: How can I make Spark logging less verbose?\n",
    "\n",
    "- By default Spark logs messages at the `INFO` level.\n",
    "\n",
    "- Here are the steps to make it only print out warnings and errors.\n",
    "    `cd $SPARK_HOME/conf`\n",
    "    `cp log4j.properties.template log4j.properties`\n",
    "    `sed -i.bak -e 's/rootCategory=INFO/rootCategory=ERROR/' log4j.properties`\n",
    "\n",
    "Spark Fundamentals\n",
    "==================\n",
    "\n",
    "Spark Execution\n",
    "---------------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n",
    "\n",
    "\n",
    "Spark Terminology\n",
    "-----------------\n",
    "\n",
    "Term                   |Meaning\n",
    "----                   |-------\n",
    "Driver                 |Process that contains the Spark Context\n",
    "Executor               |Process that executes one or more Spark tasks\n",
    "Master                 |Process which manages applications across the cluster\n",
    "                       |E.g. Spark Master\n",
    "Worker                 |Process which manages executors on a particular worker node\n",
    "                       |E.g. Spark Worker\n",
    "\n",
    "Spark Job\n",
    "---------\n",
    "\n",
    "Q: Flip a coin 100 times using Python's `random()` function. What\n",
    "fraction of the time do you get heads?\n",
    "\n",
    "- Initialize Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#only do this once. or else it will error at you! - drxt\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import random.\n",
    "\n",
    "Flip a coin 100+ xs. Do we get heads or tails?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "flips = 1000000\n",
    "heads = sc.parallelize(xrange(flips)) \\  #have to set sc.parallize to send it to executes\n",
    "    .map(lambda i: random.random()) \\\n",
    "    .count()  #just add up total number of records we have to get heads\n",
    "\n",
    "ratio = float(heads)/float(flips)\n",
    "\n",
    "print(heads)\n",
    "print(ratio)\n",
    "\n",
    "#DRXT in lecture edits\n",
    "print heads #won't be a think. have to put an action on it\n",
    "print heads.count()  #this does work\n",
    "print take(5) #first five element of your RDD\n",
    "print heads.collect()  #all the elements in your RDD. Be careful! dangerous cause its.. all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_prime(number):\n",
    "    factor_min = 2\n",
    "    factor_max = int(number**0.5)+1\n",
    "    for factor in xrange(factor_min,factor_max):\n",
    "        if number % factor == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use this to filter out non-primes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = xrange(2,100)\n",
    "primes = sc.parallelize(numbers)\\\n",
    "    .filter(is_prime)\\\n",
    "    .collect()\n",
    "print primes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does `is_prime` execute?\n",
    "</summary>\n",
    "On the executors.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does the RDD code execute?\n",
    "</summary>\n",
    "On the driver.\n",
    "</details>\n",
    "\n",
    "Transformations and Actions\n",
    "===========================\n",
    "\n",
    "Common RDD Constructors\n",
    "-----------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`sc.parallelize(list1)`                  |Create RDD of elements of list\n",
    "`sc.textFile(path)`                      |Create RDD of lines from file\n",
    "\n",
    "Common Transformations\n",
    "----------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n",
    "`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())`               |Split each string into words\n",
    "`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n",
    "`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n",
    "`union(rdd)`                             |Append `rdd` to existing RDD\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order\n",
    "\n",
    "\n",
    "Common Actions\n",
    "--------------\n",
    "\n",
    "Expression                             |Meaning\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(3)`                              |First 3 elements of RDD \n",
    "`top(3)`                               |Top 3 elements of RDD\n",
    "`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([1,3,2,2,1]).distinct().collect()  #[2,1,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.parallelize([1,3,2,2,1]).sortBy(lambda x: x).collect() # [1,1,2,2,3]\n",
    "\n",
    "#lecture example 2\n",
    "sc.parallelize([('WA', 200), ('CA',200), ('CA', 100)]).sortBy(lambda x: x, ascending = False).collect()\n",
    "#[('CA', 100),('CA', 200), ('WA', 200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?\n",
    "\n",
    "- Create this input file.\n",
    "\n",
    "- drxt this is a better way. never want the data to show up in the driver. keep it as arms length in the executer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you get when you run this code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt') \\  #RDD of lines. \n",
    "    .count()  #4\n",
    "    \n",
    "sc.textFile('input.txt') \\  #RDD of lines. \n",
    "    .map(lambda x: x.split()) \\\n",
    "    .count()  #4. But now everying is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .count()\n",
    "#2 + 2 + 3 + 4 = 11\n",
    "#flat map \n",
    "    #if we looked at it with .coolect() it would output\n",
    "    #.collect()\n",
    "    #['hello', 'world', 'another', 'line',....]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map vs FlatMap\n",
    "--------------\n",
    "\n",
    "- Here's the difference between `map` and `flatMap`.\n",
    "\n",
    "- Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .map(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FlatMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in class example 2\n",
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .flatMap(lambda x: x) \\#interpret string as a sequence and flatten it a ittle more\n",
    "    .count()\n",
    "    #.collect()\n",
    "    #['h', 'e', 'l', ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD Statistics\n",
    "--------------\n",
    "\n",
    "Q: How would you calculate the mean, variance, and standard deviation of a sample\n",
    "produced by Python's `random()` function?\n",
    "\n",
    "- Create an RDD and apply the statistical actions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 1000\n",
    "list = [random.random() for _ in xrange(count)]\n",
    "rdd = sc.parallelize(list)  #parallel the list. This is distributed across executers.\n",
    "#each eceduter gets a result of the mean, variance\n",
    "#and then the driver puts them all together.\n",
    "#so a bunch of local answers, and then the driver adds up all the answers\n",
    "print rdd.mean()\n",
    "print rdd.variance()\n",
    "print rdd.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What requirement does an RDD have to satisfy before you can apply\n",
    "these statistical actions to it? \n",
    "</summary>\n",
    "The RDD must consist of numeric elements.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the advantage of using Spark vs Numpy to calculate mean or standard deviation?\n",
    "</summary>\n",
    "The calculation is distributed across different machines and will be\n",
    "more scalable.  drxt: you know, once your data is big. so spark gives you a way to scale it up.\n",
    "</details>\n",
    "\n",
    "RDD Laziness\n",
    "------------\n",
    "\n",
    "- Q: What is this Spark job doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max = 100000\n",
    "%time sc.parallelize(xrange(max)).map(lambda x:x+1).count()\n",
    "#go through all the numbers, and add 1 to each one. \n",
    "#takes about 1 second for ten million numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q: How is the following job different from the previous one? How\n",
    "  long do you expect it to take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time sc.parallelize(xrange(max)).map(lambda x:x+1)\n",
    "\n",
    "#so now it doesn't have a thing to do. it took 3 ms. what is going on?\n",
    "#it didn't actually transform it. it just said, okay. i will do it when you ask me to. \n",
    "#will wait and do it until it is forced to give you a method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Why did the second job complete so much faster?\n",
    "</summary>\n",
    "1. Because Spark is lazy. \n",
    "<br>\n",
    "2. Transformations produce new RDDs and do no operations on the data.\n",
    "<br>\n",
    "3. Nothing happens until an action is applied to an RDD.\n",
    "<br>\n",
    "4. An RDD is the *recipe* for a transformation, rather than the\n",
    "   *result* of the transformation.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the benefit of keeping the recipe instead of the result of\n",
    "the action?\n",
    "</summary>\n",
    "1. It save memory.\n",
    "<br>\n",
    "2. It produces *resilience*. \n",
    "<br>\n",
    "3. If an RDD loses data on a machine, it always knows how to recompute it.\n",
    "</details>\n",
    "\n",
    "Writing Data\n",
    "------------\n",
    "\n",
    "Besides reading data Spark and also write data out to a file system.\n",
    "\n",
    "Q: Calculate the squares of integers from 1 to 100 and write them out\n",
    "to `squares.txt`.\n",
    "\n",
    "- Make sure `squares.txt` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!if [ -e squares.txt ] ; then rm -rf squares.txt ; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the RDD and then save it to `squares.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd1 = sc.parallelize(xrange(10))\n",
    "rdd2 = rdd1.map(lambda x: x*x)\n",
    "rdd2.saveAsTextFile('squares.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat squares.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looks like the output is a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -l squares.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take a look at the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!for i in squares.txt/part-*; do echo $i; cat $i; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What's going on? Why are there two files in the output directory?\n",
    "</summary>\n",
    "1. There were two threads that were processing the RDD.\n",
    "<br>\n",
    "2. The RDD was split up in two partitions (by default).\n",
    "<br>\n",
    "3. Each partition was processed in a different task.\n",
    "</details>\n",
    "\n",
    "Partitions\n",
    "----------\n",
    "\n",
    "Q: Can we control the number of partitions/tasks that Spark uses for\n",
    "processing data? Solve the same problem as above but this time with 5\n",
    "tasks.\n",
    "\n",
    "- Make sure `squares.txt` does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!if [ -e squares.txt ] ; then rm -rf squares.txt ; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the RDD and then save it to `squares.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partitions = 5  #instead of 2, what got last time\n",
    "rdd1 = sc.parallelize(xrange(10), partitions)\n",
    "rdd2 = rdd1.map(lambda x: x*x)\n",
    "rdd2.saveAsTextFile('squares.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls -l squares.txt\n",
    "!for i in squares.txt/part-*; do echo $i; cat $i; done\n",
    "\n",
    "'''five files, each with two elements. The number of partitions to divide your data into \n",
    "0\n",
    "1\n",
    "___\n",
    "4\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How many partitions does Spark use by default?\n",
    "</summary>\n",
    "1. By default Spark uses 2 partitions.\n",
    "<br>\n",
    "2. If you read an HDFS file into an RDD Spark uses one partition per\n",
    "   block.\n",
    "<br>\n",
    "3. If you read a file into an RDD from S3 or some other source Spark\n",
    "   uses 1 partition per 32 MB of data.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: If I read a file that is 200 MB into an RDD, how many partitions will that have?\n",
    "</summary>\n",
    "1. If the file is on HDFS that will produce 2 partitions (each is 128\n",
    "   MB).\n",
    "<br>\n",
    "2. If the file is on S3 or some other file system it will produce 7\n",
    "   partitions.\n",
    "<br>\n",
    "3. You can also control the number of partitions by passing in an\n",
    "   additional argument into `textFile`.\n",
    "</details>\n",
    "\n",
    "Spark Terminology\n",
    "-----------------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n",
    "\n",
    "Term                   |Meaning\n",
    "----                   |-------\n",
    "Task                   |Single thread in an executor\n",
    "Partition              |Data processed by a single task\n",
    "Record                 |Records make up a partition that is processed by a single task\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- Every Spark application gets executors when you create a new `SparkContext`.\n",
    "\n",
    "- You can specify how many cores to assign to each executor.\n",
    "\n",
    "- A core is equivalent to a thread.\n",
    "\n",
    "- The number of cores determine how many tasks can run concurrently on\n",
    "  an executor.\n",
    "\n",
    "- Each task corresponds to one partition.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Suppose you have 2 executors, each with 2 cores--so a total of 4\n",
    "cores. And you start a Spark job with 8 partitions. How many tasks\n",
    "will run concurrently?\n",
    "</summary>\n",
    "4 tasks will execute concurrently.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What happens to the other partitions?\n",
    "</summary>\n",
    "1. The other partitions wait in queue until a task thread becomes\n",
    "available.\n",
    "<br>\n",
    "2. Think of cores as turnstile gates at a train station, and\n",
    "   partitions as people .\n",
    "<br>\n",
    "3. The number of turnstiles determine how many people can get through\n",
    "   at once.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: How many Spark jobs can you have in a Spark application?\n",
    "</summary>\n",
    "As many as you want.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: How many Spark applications and Spark jobs are in this IPython Notebook?\n",
    "</summary>\n",
    "1. There is one Spark application because there is one `SparkContext`.\n",
    "<br>\n",
    "2. There are as many Spark jobs as we have invoked actions on RDDs.\n",
    "</details>\n",
    "\n",
    "Stock Quotes\n",
    "------------\n",
    "\n",
    "Q: Find the date on which AAPL's stock price was the highest.\n",
    "\n",
    "Suppose you have stock market data from Yahoo! for AAPL from\n",
    "<http://finance.yahoo.com/q/hp?s=AAPL+Historical+Prices>. The data is\n",
    "in CSV format and has these values.\n",
    "\n",
    "Date        |Open    |High    |Low     |Close   |Volume      |Adj Close\n",
    "----        |----    |----    |---     |-----   |------      |---------\n",
    "11-18-2014  |113.94  |115.69  |113.89  |115.47  |44,200,300  |115.47\n",
    "11-17-2014  |114.27  |117.28  |113.30  |113.99  |46,746,700  |113.99\n",
    "\n",
    "Here is what the CSV looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = [\n",
    "  \"#Date,Open,High,Low,Close,Volume,Adj Close\\n\",\n",
    "  \"2014-11-18,113.94,115.69,113.89,115.47,44200300,115.47\\n\",\n",
    "  \"2014-11-17,114.27,117.28,113.30,113.99,46746700,113.99\\n\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the date on which the price was the highest. \n",
    "\n",
    "\n",
    "<details><summary>\n",
    "Q: What two fields do we need to extract? \n",
    "</summary>\n",
    "1. *Date* and *Adj Close*.\n",
    "<br>\n",
    "2. We want to use *Adj Close* instead of *High* so our calculation is\n",
    "   not affected by stock splits.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What field should we sort on?\n",
    "</summary>\n",
    "*Adj Close*\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What sequence of operations would we need to perform?\n",
    "</summary>\n",
    "1. Use `filter` to remove the header line.\n",
    "<br>\n",
    "2. Use `map` to split each row into fields.\n",
    "<br>\n",
    "3. Use `map` to extract *Adj Close* and *Date*.\n",
    "<br>\n",
    "4. Use `sortBy` to sort descending on *Adj Close*.\n",
    "<br>\n",
    "5. Use `take(1)` to get the highest value.\n",
    "</details>\n",
    "\n",
    "- Here is full source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv = [\n",
    "  \"#Date,Open,High,Low,Close,Volume,Adj Close\\n\",\n",
    "  \"2014-11-18,113.94,115.69,113.89,115.47,44200300,115.47\\n\",\n",
    "  \"2014-11-17,114.27,117.28,113.30,113.99,46746700,113.99\\n\",\n",
    "]\n",
    "sc.parallelize(csv) \\\n",
    "  .filter(lambda line: not line.startswith(\"#\")) \\\n",
    "  .map(lambda line: line.split(\",\")) \\\n",
    "  .map(lambda fields: (float(fields[-1]),fields[0])) \\\n",
    "  .sortBy(lambda (close, date): close, ascending=False)\n",
    "  .take(1)\n",
    "\n",
    "#sort sorts by all the executers on each task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the program for finding the high of any stock that stores\n",
    "  the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import re\n",
    "\n",
    "def get_stock_high(symbol):\n",
    "  url = 'http://real-chart.finance.yahoo.com' + \\\n",
    "    '/table.csv?s='+symbol+'&g=d&ignore=.csv'\n",
    "  csv = urllib2.urlopen(url).read()\n",
    "  csv_lines = csv.split('\\n')\n",
    "  stock_rdd = sc.parallelize(csv_lines) \\\n",
    "    .filter(lambda line: re.match(r'\\d', line)) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda fields: (float(fields[-1]),fields[0])) \\\n",
    "    .sortBy(lambda (close, date): close, ascending=False)\n",
    "  return stock_rdd.take(1)\n",
    "\n",
    "get_stock_high('AAPL')\n",
    "\n",
    "#[(75, '2015-06-18')]\n",
    "\n",
    "#worst case, you could inject boto connections\n",
    "#each mappers coudl work on a subset of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes\n",
    "-----\n",
    "\n",
    "- Spark is high-level like Hive and Pig.\n",
    "\n",
    "- At the same time it does not invent a new language.\n",
    "\n",
    "- This allows it to leverage the ecosystem of tools that Python,\n",
    "  Scala, and Java provide.\n",
    "\n",
    "Key Value Pairs\n",
    "===============\n",
    "\n",
    "PairRDD\n",
    "-------\n",
    "\n",
    "At this point we know how to aggregate values across an RDD. If we\n",
    "have an RDD containing sales transactions we can find the total\n",
    "revenue across all transactions.\n",
    "\n",
    "Q: Using the following sales data find the total revenue across all\n",
    "transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .take(2)\n",
    "    \n",
    "#just write it incramentally, so you can immedately catch any issues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: x[0].startswith('#'))\\ #take out header\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\  #he is just showing how it goes\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick off last field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: x[-1])\\\n",
    "    .take(2)\n",
    "    \n",
    "#great for development, always move incramentaly towards your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to float and then sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#want to look the amount, which is the last feaild. so extract that out\n",
    "#then apply float to numbers\n",
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: float(x[-1]))\\\n",
    "    .sum()\n",
    "    \n",
    "#2230"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceByKey\n",
    "-----------\n",
    "\n",
    "Q: Calculate revenue per state?\n",
    "\n",
    "- Instead of creating a sequence of revenue numbers we can create\n",
    "  tuples of states and revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .collect()\n",
    "#[('WA', 300),\n",
    "#    ('OR', 450), \n",
    "#    ect]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now use `reduceByKey` to add them up.\n",
    "\n",
    "if tuples contain three elements, or one, it won't work. HAS to be two. -drxt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Find the state with the highest total revenue.\n",
    "\n",
    "- You can either use the action `top` or the transformation `sortBy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What does `reduceByKey` do?\n",
    "</summary>\n",
    "1. It is like a reducer.\n",
    "<br>\n",
    "2. If the RDD is made up of key-value pairs, it combines the values\n",
    "   across all tuples with the same key by using the function we pass\n",
    "   to it.\n",
    "<br>\n",
    "3. It only works on RDDs made up of key-value pairs or 2-tuples.\n",
    "</details>\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- `reduceByKey` only works on RDDs made up of 2-tuples.\n",
    "\n",
    "- `reduceByKey` works as both a reducer and a combiner.\n",
    "\n",
    "- It requires that the operation is associative.\n",
    "\n",
    "Word Count\n",
    "----------\n",
    "\n",
    "Q: Implement word count in Spark.\n",
    "\n",
    "- Create some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('input.txt')\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: (word,1))\\\n",
    "    .reduceByKey(lambda count1,count2: count1+count2)\\\n",
    "    .collect()\n",
    "    \n",
    "#[u('line', 3), (u'another', 5)...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making List Indexing Readable\n",
    "-----------------------------\n",
    "\n",
    "- While this code looks reasonable, the list indexes are cryptic and\n",
    "  hard to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can make this more readable using Python's argument unpacking\n",
    "  feature.\n",
    "\n",
    "Argument Unpacking\n",
    "------------------\n",
    "\n",
    "Q: Which version of `getCity` is more readable and why?\n",
    "\n",
    "- Consider this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = ('Dmitri','Smith','SF')\n",
    "\n",
    "def getCity1(client):\n",
    "    return client[2]\n",
    "\n",
    "def getCity2((first,last,city)):\n",
    "    return city\n",
    "\n",
    "print getCity1(client)\n",
    "\n",
    "print getCity2(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the difference between `getCity1` and `getCity2`?\n",
    "\n",
    "- Which is more readable?\n",
    "\n",
    "- What is the essence of argument unpacking?\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "<details><summary>\n",
    "Q: Can argument unpacking work for deeper nested structures?\n",
    "</summary>\n",
    "Yes. It can work for arbitrarily nested tuples and lists.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: How would you write `getCity` given \n",
    "`client = ('Dmitri','Smith',('123 Eddy','SF','CA'))`\n",
    "</summary>\n",
    "`def getCity((first,last,(street,city,state))): return city`\n",
    "</details>\n",
    "\n",
    "Argument Unpacking\n",
    "------------------\n",
    "\n",
    "- Lets test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SF'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = ('Dmitri','Smith',('123 Eddy','SF','CA'))\n",
    "\n",
    "def getCity((first,last,(street,city,state))):\n",
    "    return city\n",
    "\n",
    "getCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Whenever you find yourself indexing into a tuple consider using\n",
    "  argument unpacking to make it more readable.\n",
    "\n",
    "- Here is what `getCity` looks like with tuple indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SF'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def badGetCity(client):\n",
    "    return client[2][1]\n",
    "\n",
    "badGetCity(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument Unpacking In Spark\n",
    "---------------------------\n",
    "\n",
    "Q: Rewrite the last Spark job using argument unpacking.\n",
    "\n",
    "- Here is the original version of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the code with argument unpacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda (id,date,store,state,product,amount): (state,float(amount)))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda (state,amount):amount,ascending=False) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case because we have a long list or tuple argument unpacking\n",
    "  is a judgement call.\n",
    "\n",
    "GroupByKey\n",
    "----------\n",
    "\n",
    "`reduceByKey` lets us aggregate values using sum, max, min, and other\n",
    "associative operations. But what about non-associative operations like\n",
    "average? How can we calculate them?\n",
    "\n",
    "- There are several ways to do this.\n",
    "\n",
    "- The first approach is to change the RDD tuples so that the operation\n",
    "  becomes associative. \n",
    "\n",
    "- Instead of `(state, amount)` use `(state, (amount, count))`.\n",
    "\n",
    "- The second approach is to use `groupByKey`, which is like\n",
    "  `reduceByKey` except it gathers together all the values in an\n",
    "  iterator. \n",
    "  \n",
    "- The iterator can then be reduced in a `map` step immediately after\n",
    "  the `groupByKey`.\n",
    "\n",
    "Q: Calculate the average sales per state.\n",
    "\n",
    "- Approach 1: Restructure the tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],(float(x[-1]),1)))\\\n",
    "    .reduceByKey(lambda (amount1,count1),(amount2,count2): \\\n",
    "        (amount1+amount2, count1+count2))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note the argument unpacking we are doing in `reduceByKey` to name\n",
    "  the elements of the tuples.\n",
    "\n",
    "- Approach 2: Use `groupByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean(iter):\n",
    "    total = 0.0; count = 0\n",
    "    for x in iter:\n",
    "        total += x; count += 1\n",
    "    return total/count\n",
    "\n",
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .groupByKey() \\\n",
    "    .map(lambda (state,iter): mean(iter))\\   #wrote his own mean because numpy won't work with iterators\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that we are using unpacking again.\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What would be the disadvantage of not using unpacking?\n",
    "</summary>\n",
    "1. We will need to drill down into the elements.\n",
    "<br>\n",
    "2. The code will be harder to read.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: What are the pros and cons of `reduceByKey` vs `groupByKey`?\n",
    "</summary>\n",
    "1. `groupByKey` stores the values for particular key as an iterable.\n",
    "<br>\n",
    "2. This will take up space in memory or on disk.\n",
    "<br>\n",
    "3. `reduceByKey` therefore is more scalable.\n",
    "<br>\n",
    "4. However, `groupByKey` does not require associative reducer\n",
    "   operation.\n",
    "<br>\n",
    "5. For this reason `groupByKey` can be easier to program with.\n",
    "</details>\n",
    "\n",
    "<!--\n",
    "\n",
    "Find Highest Revenue State\n",
    "\n",
    "Cache and Persist\n",
    "\n",
    "Checkpoint\n",
    "\n",
    "Narrow vs Wide Operations\n",
    "\n",
    "Partitions\n",
    "\n",
    "Broadcast Variables\n",
    "\n",
    "Accumulators\n",
    "\n",
    "x = sc.parallelize(xrange(10)).filter(lambda x: x % 2 == 0).collect()\n",
    "\n",
    "x = sc.parallelize([1,2,3,4,5,5,1,3]).distinct().collect()\n",
    "\n",
    "x = sc.parallelize([1,2,3,4,5,5,1,3]).mean()\n",
    "\n",
    "x = sc.parallelize([1,2,3,4,5,5,1,3]).mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Joins\n",
    "Q: Given a table of employees and locations find the cities that the employees live in.\n",
    " - The easiest way to do this is with a join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Employees: emp_id, loc_id, name\n",
    "employee_data = [\n",
    "    (101, 014, 'Alice'),\n",
    "    (102, 015, 'Bob'),\n",
    "    (103, 014, 'Chad'),\n",
    "    (104, 015, 'Jen'),\n",
    "    (105, 013, 'Dee') ]\n",
    "\n",
    "# Locations: loc_id, location\n",
    "location_data = [\n",
    "    (014, 'SF'),\n",
    "    (015, 'Seattle'),\n",
    "    (016, 'Portland')]\n",
    "\n",
    "employees = sc.parallelize(employee_data)\n",
    "locations = sc.parallelize(location_data)\n",
    "\n",
    "# Re-key employee records with loc_id\n",
    "employees2 = employees.map(lambda (emp_id,loc_id,name):(loc_id,name));\n",
    "\n",
    "# Now join.\n",
    "employees2.join(locations).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PopQuiz:\n",
    "\n",
    "How do we keep employees that don't have a valid location ID in the final result?\n",
    " - Use `leftOuterJoin` to keep employees without location ID\n",
    " - `rightOuterJoin` to keep locations without employees\n",
    " - `fullOuterJoin` to keep both\n",
    "\n",
    "##Caching and Persistence\n",
    "###RDD Caching\n",
    " - Consider this Spark job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max = 100000\n",
    "%time rdd1 = sc.parallelize(xrange(max))\n",
    "%time rdd2 = rdd1.map(lambda x:x*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Until we execute an action the RDDs do nothing.\n",
    " - Now lets force execution of the RDD by calling an action on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time c = rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Notice that Spark is not caching rdd1 or rdd2.\n",
    " - The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throw away all the data.\n",
    " - If you have an RDD that you are going to reuse in your computation you can use cache() to make Spark cache the RDD.\n",
    " \n",
    "###RDD Caching\n",
    "Q: Repeat the previous computation with cache enabled on the RDDs.\n",
    " - Set up the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max = 100000\n",
    "rdd1 = sc.parallelize(xrange(max))\n",
    "rdd2 = rdd1.map(lambda x:x*x)\n",
    "\n",
    "#enable caching on rdd1\n",
    "rdd1.cache();  #if it ever has the data it will hold up on it\n",
    "\n",
    "#Observe the performance - not much better. Still have to go through the data\n",
    "%time rdd2.count()\n",
    "\n",
    "#now cache rdd2\n",
    "rdd2.cache();\n",
    "\n",
    "#now when we run it, \n",
    "%time rdd2.count()\n",
    "#4.91 sec to 1.65 seconds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in class notes\n",
    "\n",
    "rdd1 = sc.parallelize([random.random() for _ in xrange(1000000)])\n",
    "rdd2 = rdd1.sortBy(lambda x:x)\n",
    "rdd2.cache()\n",
    "rdd3 = rdd2.filter(lambda x: x>0.5)\n",
    "\n",
    "%time rdd3.count()\n",
    "'''wall time: 344ms\n",
    "rdd2 is caches\n",
    "if run it again, it drops down to 57.6ms\n",
    "'''%time rdd3.count()\n",
    "'''So if use an rdd multiple times, you want to cache it\n",
    "or else you will ahve to do it every time'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Notes\n",
    " - Calling `cache()` flips a flag on the RDD.\n",
    " - The data is not cached until an action is called.\n",
    " - You can uncache an RDD using `unpersist()`.\n",
    " \n",
    "#####Pop Quiz:\n",
    "Will `unpersist` uncache the RDD immediately or does it wait for an action?\n",
    " \n",
    "###Caching and Persistence\n",
    "Q: Persist RDD to disk instead of caching it in memory.\n",
    " - You can cache RDDs at different levels.\n",
    " - Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "rdd = sc.parallelize(xrange(100))\n",
    "rdd.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "everytime you use the RDD it will be read from disk. \n",
    "\n",
    "Will the RDD be stored on disk when you exectue it?\n",
    "No. You have to call some kind of an action for the persistance to act on it. \n",
    "\n",
    "\n",
    "###Persistence Levels\n",
    "\n",
    "-----\n",
    "Level/Meaning\n",
    "- `MEMORY_ONLY` Same as `cache()` \n",
    "- `MEMORY_AND_DISK` Cache in memory then overflow to disk \n",
    "- `MEMORY_AND_DISK_SER` Like above; in cache keep objects serialized instead of live  \n",
    "- `DISK_ONLY` Cache to disk not to memory\n",
    "\n",
    "Notes\n",
    "----\n",
    " - `MEMORY_AND_DISK_SER` is a good compromise between the levels.\n",
    " - Fast, but not too expensive.\n",
    " - Make sure you unpersist when you don't need the RDD any more.\n",
    "\n",
    "##Spark Performance\n",
    "###Narrow and Wide Transformations\n",
    " - Spark transformations are narrow if each RDD has one unique child past the transformation.\n",
    " - Spark transformations are wide if each RDD can have multiple children past the transformation.\n",
    " - Narrow transformations are map-like, while wide transformations are reduce-like.\n",
    " - Narrow transformations are faster because they do move data between executors, while wide transformations are slower.\n",
    "\n",
    "- drxt:join, reduced_bykey , make sure get as little data as possible because its going to go between all the comp\n",
    "\n",
    "###Repartitioning\n",
    " - Over time partitions can get skewed.\n",
    " - Or you might have less data or more data than you started with.\n",
    " - You can rebalance your partitions using `repartition` or `coalesce`.\n",
    " - `coalesce  is narrow while repartition is wide.\n",
    "\n",
    "drxt notes:\n",
    " - when wide, have the opportunity to ask for new partitions. \n",
    " - Wide operations like sortByKey, have a second argument for number of partitions\n",
    " - so if reduceByKey, maybe don't need 100 partitions anymore, so you can tell it to give you less. Same with join. \n",
    " \n",
    " ALL WIDE OPERATIONS take the number of partitions\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psypark\n",
    "rdd = sc.parallelize(xrange(100))\n",
    "rdd.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Misc\n",
    "####Amazon S3\n",
    " - \"s3:\" URLs break when Secret Key contains a slash, even if encoded https://issues.apache.org/jira/browse/HADOOP-3733\n",
    " - Spark 1.3.1 / Hadoop 2.6 prebuilt pacakge has broken S3 filesystem access https://issues.apache.org/jira/browse/SPARK-7442"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###guidelines for partitions\n",
    "\n",
    " - if taking less than 120ms, probably too  small \n",
    " - 30 - 100 MB of data. So if smaller, just wasting overhead.\n",
    " - If super larger, will be bogged down in tryign to process all the records\n",
    " \n",
    "`coalesce()` --> good to reduce the number of partitions, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
