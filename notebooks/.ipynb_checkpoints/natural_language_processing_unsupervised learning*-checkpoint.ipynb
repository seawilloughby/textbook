{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TF-IDF \n",
    "####term frequency inverse document frequency\n",
    "How frequently is each word over the documents? Relevance Measure\n",
    "\n",
    "How to evlaute? Its unsupervised sooo.\n",
    "Compare it to something else?\n",
    "\n",
    "- Other NLP\n",
    "    - POS-Tagging\n",
    "    - word2vec\n",
    "    - Stemming e.g. hosting/host, lying/lie etc.\n",
    "    - chunking\n",
    "    - grammars\n",
    "    - semantic orientation\n",
    "    \n",
    "##Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ,  0.        ,  0.        ,  0.40824829,  0.        ,\n",
       "          0.        ,  0.81649658,  0.        ,  0.40824829,  0.        ],\n",
       "        [ 0.        ,  0.41137791,  0.        ,  0.        ,  0.64450299,\n",
       "          0.64450299,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.5417361 ,  0.34578314,  0.5417361 ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.5417361 ],\n",
       "        [ 0.        ,  0.53802897,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.84292635,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [\"Dogs like dogs more than cats.\",\n",
    "             \"The dog chased the bicycle.\",\n",
    "             \"The cat rode in the bicycle basket.\",\n",
    "             \"I have a fast bicycle.\"]\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorized = vect.fit_transform(documents)\n",
    "\n",
    "#to see the tfidf:\n",
    "#this is because this is a sparse matrix, and it has been collapsed since there can be lots of zeros\n",
    "tfidf_vectorized.todense(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-454e416c5d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#this is because this is a sparse matrix, and it has been collapsed since there can be lots of zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtfidf_vectorized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mlinear_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vectorized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_kernel' is not defined"
     ]
    }
   ],
   "source": [
    "documents = [\"Dogs like cats.\",\n",
    "             \"Cats like apples.\",\n",
    "             \"Doggy sleepy.\"]\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorized = vect.fit_transform(documents)\n",
    "\n",
    "#to see the tfidf:\n",
    "#this is because this is a sparse matrix, and it has been collapsed since there can be lots of zeros\n",
    "tfidf_vectorized.todense(0)\n",
    "linear_kernel(tfidf_vectorized, tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'apples': 0, u'cats': 1, u'doggy': 2, u'dogs': 3, u'like': 4, u'sleepy': 5}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we have vectors, we can measure thier similarity\n",
    "###Cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  1.        ,  0.14224755,  0.22133323],\n",
       "       [ 0.        ,  0.14224755,  1.        ,  0.18604135],\n",
       "       [ 0.        ,  0.22133323,  0.18604135,  1.        ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "documents = [\"Dogs like dogs more than cats.\",\n",
    "             \"The dog chased the bicycle.\",\n",
    "             \"The cat rode in the bicycle basket.\",\n",
    "             \"I have a fast bicycle.\"]\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "tfidf_vectorized = vect.fit_transform(documents)\n",
    "\n",
    "cosine_similarities = linear_kernel(tfidf_vectorized, tfidf_vectorized)\n",
    "cosine_similarities  #documents 2 and 4 are most similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterate over all possible pairs (as in 2 for loops iterating over the same list of documents) print the cosine similarities of their tfidf scores for each documents bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1.0\n",
      "1 2 0.0\n",
      "1 3 0.0\n",
      "1 4 0.0\n",
      "2 1 0.0\n",
      "2 2 1.0\n",
      "2 3 0.142247545163\n",
      "2 4 0.221333233547\n",
      "3 1 0.0\n",
      "3 2 0.142247545163\n",
      "3 3 1.0\n",
      "3 4 0.186041345374\n",
      "4 1 0.0\n",
      "4 2 0.221333233547\n",
      "4 3 0.186041345374\n",
      "4 4 1.0\n"
     ]
    }
   ],
   "source": [
    "#a simple forloop to print what I have above.\n",
    "for i, doc1 in enumerate(tfidf_vectorized): #or docs\n",
    "    for j, doc2 in enumerate(tfidf_vectorized): #or 'docs'\n",
    "        print i+1, j+1, cosine_similarities[i, j]  #+1 because of 0 indexing to get doc num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##More Detail:\n",
    "###How can I fus to get a great corpus formatted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'bicycle': 1, u'like': 8, u'chased': 4, u'rode': 9, u'dog': 5, u'fast': 7, u'cat': 2, u'cats': 3, u'basket': 0, u'dogs': 6}\n",
      "\n",
      "Example Term Frequency \n",
      ": [[0 0 0 1 0 0 2 0 1 0]\n",
      " [0 1 0 0 1 1 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 1]\n",
      " [0 1 0 0 0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#in this case, a corpus is four sentences. Each sentence is a 'document'\n",
    "corpus = [\"Dogs like dogs more than cats.\",\n",
    "             \"The dog chased the bicycle.\",\n",
    "             \"The cat rode in the bicycle basket.\",\n",
    "             \"I have a fast bicycle.\"]\n",
    "\n",
    "#this function turns each sentence into a bag of words for us!!! \n",
    "countvect = CountVectorizer(stop_words='english')\n",
    "c = countvect.fit(corpus)\n",
    "#see bottom if you want to trucate words, get them into a better format\n",
    "#vocabulary is ALL the words in a corpus\n",
    "print c.vocabulary_\n",
    "\n",
    "tf = c.transform(corpus).todense()\n",
    "print '\\nExample Term Frequency \\n:', tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.        ,  0.        ,  0.        ,  1.91629073,  0.        ,\n",
       "          0.        ,  3.83258146,  0.        ,  1.91629073,  0.        ],\n",
       "        [ 0.        ,  1.22314355,  0.        ,  0.        ,  1.91629073,\n",
       "          1.91629073,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 1.91629073,  1.22314355,  1.91629073,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  1.91629073],\n",
       "        [ 0.        ,  1.22314355,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  1.91629073,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to use this homemade TF matrix to a TIDF: \n",
    "z = TfidfTransformer(norm=None)\n",
    "z.fit_transform(tf).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###how do we get raw text into a nice Bag of Words for TF-IDG?\n",
    "this is an example from our assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "can't assign to function call (<ipython-input-73-23075682b492>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-73-23075682b492>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    tokens = [word_tokenize(sent) for sent.lower() in corpus]\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m can't assign to function call\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "corpus = ['Dogs are super cute and antsy', 'my dog spins in circles.']\n",
    "tokens = [word_tokenize(sent) for sent.lower() in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1) remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "#also i lowercased it because i wanted to, and removed punctuation\n",
    "from string import punctuation\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    tokens_trimmed= []\n",
    "    for document in tokens:\n",
    "        stopwrds = stopwords.words('english')\n",
    "        \n",
    "        #don't include stopwords\n",
    "        content = [w for w in document if w.lower() not in stopwrds]\n",
    "        \n",
    "        #extra: remove punctuation and lowercase the words. BONUS\n",
    "        content = [w.lower() for w in document if w not in punctuation]\n",
    "        \n",
    "        tokens_trimmed.append(content)\n",
    "    return tokens_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-117ee47dfd93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens_trimmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokens_trimmed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "tokens_trimmed = remove_stopwords(tokens)\n",
    "tokens_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####2) stemmling/lemmatization. Three different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def port_stem_it(document):\n",
    "    '''Takes a document, returns the word stems in a list.'''\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(plural) for plural in document]\n",
    "\n",
    "def snowball_stem_it(document):\n",
    "    '''Takes a document, returns the word stems in a list.\n",
    "    DOES turn everything into unicode'''\n",
    "    snowball = SnowballStemmer('english')\n",
    "    return [snowball.stem(word) for word in document]\n",
    "\n",
    "def wornet_lemmit(document):\n",
    "    '''Takes a document, returns the word stems in a list.'''\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    return [wordnet.lemmatize(word) for word in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'are', 'super', 'cute', 'and', 'antsi']\n"
     ]
    }
   ],
   "source": [
    "#example of how lemming works:\n",
    "print port_stem_it(tokens_trimmed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dog', 'are', 'super', 'cute', 'and', 'antsi'],\n",
       " ['my', 'dog', 'spin', 'in', 'circl']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_trim = []\n",
    "for document in tokens_trimmed:\n",
    "    doc = port_stem_it(document)\n",
    "    tokens_trim.append(doc)\n",
    "\n",
    "#now these two documents are all ready to be turned into bag of words!     \n",
    "tokens_trim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###One more Example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "EXAMPLE\n",
    "X= emails\n",
    "Y = if spam or not\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(documents, y)\n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_train = vect.fit_transform(docs_train)\n",
    "X_test = vect.transform(docs_test)\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "print \"Accuracy on test set:\", log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###N Grams\n",
    "Grams are strings of word sequences. NGrams can be individual tokens or multi word sequences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'rain', 'in', 'Spain', 'falls', 'mainly')\n",
      "('rain', 'in', 'Spain', 'falls', 'mainly', 'on')\n",
      "('in', 'Spain', 'falls', 'mainly', 'on', 'the')\n",
      "('Spain', 'falls', 'mainly', 'on', 'the', 'plain')\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "sentence = 'the rain in Spain falls mainly on the plain'\n",
    "n = 6\n",
    "sixgrams = ngrams(sentence.split(), n)\n",
    "for grams in sixgrams:\n",
    "  print grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.333333333333\n",
      "  (0, 7)\t0.333333333333\n",
      "  (0, 6)\t0.333333333333\n",
      "  (0, 5)\t0.333333333333\n",
      "  (0, 4)\t0.333333333333\n",
      "  (0, 3)\t0.333333333333\n",
      "  (0, 2)\t0.333333333333\n",
      "  (0, 1)\t0.333333333333\n",
      "  (0, 0)\t0.333333333333\n"
     ]
    }
   ],
   "source": [
    "#use with vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                             stop_words='english',\n",
    "                             ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform([sentence])\n",
    "print X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####You can also do SkipGrams. \n",
    "the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences for the sentence:\n",
    "```\n",
    "The rain in spain falls mainly on the plane\n",
    "The rain, in falls, Spain mainly, mainly the and on plane\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk.data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the probability that a word drawn at random from all of the documents in a given class will be a given word (with a laplace smoothing).\n",
    " \n",
    " We are going to be calculating the \"probability\" that an article belongs to each class. It's important to note that this will not be a true probability, but it will enable us to rank the classes to figure out which is the most likely.\n",
    "\n",
    "Here's Bayes Rule applied to our scenario. x here is a word, c is the class (article topic). X is the document text of a specific document. The xi's are all the words in the given document.\n",
    "\n",
    "We make a BIG assumption here that the occurrences of each word are independent. If they weren't, we wouldn't be able to just multiply all the individual word probabilities together to get the whole document probability. Hence the \"Naive\" in Naive Bayes.\n",
    "\n",
    "Take a moment to think of how absurd this is. A document that contains the word \"soccer\" is probably more likely to contain the word \"ball\" than an average document. We assume that this is not true! But it turns out that this assumption doesn't hurt us too much and we still have a lot of predictive power. But the probabilities that we end up with are not at all true probabilities. We can only use them to compare which of the classes is the most likely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Why?\n",
    "Independence Assumption\n",
    "Generative algorithm\n",
    "Very performant... it's just counting!\n",
    "Learns online by processing one data point at a time\n",
    "see above (it is online and does incremental processing) ^^\n",
    "Each class has a different parameterization (through CPTs)\n",
    "\n",
    "When?\n",
    "n << p (# of features)   - with text, there are often lots more features\n",
    "n somewhat small or n quite large\n",
    "streams of input data (online learning)\n",
    "not bounded by memory (usually)\n",
    "Multi-class\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Unicode Help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "<type 'unicode'>\n",
      "<type 'str'>\n",
      "<type 'unicode'>\n",
      "<type 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "u'abcdef'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicode('abcdef')\n",
    "u'abcdef'\n",
    "\n",
    "s = unicode('abcdef')\n",
    "print type(s)\n",
    "\n",
    "#stayed the same\n",
    "print type(s.replace(\"abc\",\"123\"))\n",
    "\n",
    "#converts to byte string\n",
    "print type(str(s))\n",
    "\n",
    "#converts to unicode\n",
    "print type(s.decode('utf8'))\n",
    "\n",
    "#encode unicode to bytestring\n",
    "print type(s.encode())\n",
    "\n",
    "def to_unicode(word):\n",
    "   try:\n",
    "     return word.encode('latin-1').decode('utf8')\n",
    "   except:\n",
    "     return ''\n",
    "\n",
    "to_unicode(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Another Example from the Pair Assignment\n",
    "Here we will use the NYT to analyze and predict categoies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import nltk.data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = ['comp.graphics', 'rec.sport.baseball', 'sci.med', \\\n",
    "              'talk.politics.misc']\n",
    "data = fetch_20newsgroups(subset='train', categories=categories).data\n",
    "\n",
    "#this is cool the data is already a list of documents! or a corpus. Great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Subject: Let it be Known\\nFrom: <ISSBTL@BYUVM.BITNET>\\nOrganization: Brigham Young University\\nLines: 10\\n\\nI would like to make everyone aware that in winning the NL West the Atlanta\\nBraves did not lead wire-to-wire.  Through games of 4/14/93 the Houston\\nAstros are percentage points ahead of the \"unbeatable\" Braves.\\n\\n\\nGo Astros!!!!!\\n\\nByron T. Lee\\nA Native Texan\\nStuck in Utah\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#each data is an article data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectors = vectorizer.fit_transform(data).toarray()\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    INPUT: LIST, INTEGER, LIST\n",
    "    OUTPUT: LIST\n",
    "\n",
    "    Given a list of values, find the indices with the highest n values.\n",
    "    Return the labels for each of these indices.\n",
    "\n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = [\"cat\", \"dog\", \"mouse\", \"pig\", \"rabbit\"]\n",
    "    output: [\"cat\", \"pig\"]\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[-1:-n-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 by average tf-idf\n",
      "[u'xxxx', u'narrative', u'clubbing', u'compartment', u'bram', u'sphinx', u'hernia', u'comarow', u'rolandi', u'kewageshig']\n"
     ]
    }
   ],
   "source": [
    "avg = np.sum(vectors, axis=0) / np.sum(vectors > 0, axis=0)\n",
    "print \"top 10 by average tf-idf\"\n",
    "print get_top_values(avg, 10, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 by tf across all corpus\n",
      "[u'tilting', u'transfusions', u'opening', u'anecdotal', u'indivicual', u'jad', u'tiling', u'jazz', u'fought', u'egyptian']\n"
     ]
    }
   ],
   "source": [
    "#highest TF score across corpus:\n",
    "# redo vectorization without using idf\n",
    "vectorizer2 = TfidfVectorizer(use_idf=False)\n",
    "# make documents into one giant document for this purpose\n",
    "vectors2 = vectorizer2.fit_transform([\"\\n\".join(data)]).toarray()\n",
    "print \"top 10 by tf across all corpus\"\n",
    "print get_top_values(vectors2[0], 10, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: alt.atheism\n",
      "  Top 10 by average tf-idf\n",
      "    enviroleague, dlb, vonnegut, b12, tyre, racism, ites, rb, maine, bye\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, keith, god, people, caltech, writes, don, sgi, livesey\n",
      "-----------------------------\n",
      "Category: comp.graphics\n",
      "  Top 10 by average tf-idf\n",
      "    xxxx, sphinx, p_c, kewageshig, siemens, sink, bates, bockamp, stereoscopic, newcastle\n",
      "  Top 10 by total tf-idf\n",
      "    edu, graphics, com, lines, subject, organization, university, posting, host, nntp\n",
      "-----------------------------\n",
      "Category: comp.os.ms-windows.misc\n",
      "  Top 10 by average tf-idf\n",
      "    donoghue, ramirez, osburn, maley, drum, bekker, toelle, ax, srini, schwenk\n",
      "  Top 10 by total tf-idf\n",
      "    edu, windows, com, file, dos, lines, subject, organization, university, ax\n",
      "-----------------------------\n",
      "Category: comp.sys.ibm.pc.hardware\n",
      "  Top 10 by average tf-idf\n",
      "    netbios, dh, interliving, gamecards, ren, btr, erlangen, harding, satam, oracle\n",
      "  Top 10 by total tf-idf\n",
      "    edu, scsi, com, drive, ide, card, lines, subject, bus, organization\n",
      "-----------------------------\n",
      "Category: comp.sys.mac.hardware\n",
      "  Top 10 by average tf-idf\n",
      "    eu, nestor, bmug, missouri, gnd, hsieh, meharg, scot, gpb, arie\n",
      "  Top 10 by total tf-idf\n",
      "    edu, mac, apple, com, lines, subject, drive, organization, university, posting\n",
      "-----------------------------\n",
      "Category: comp.windows.x\n",
      "  Top 10 by average tf-idf\n",
      "    plplot, lakshman, mufti, fzi, devguide, xclrp, banz, fabian, baldwa, leon\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, window, motif, lines, mit, server, subject, organization, host\n",
      "-----------------------------\n",
      "Category: misc.forsale\n",
      "  Top 10 by average tf-idf\n",
      "    maine, forman, damico, konrad, ehud, gt4661a, spine, sipps, munroe, nevada\n",
      "  Top 10 by total tf-idf\n",
      "    edu, sale, 00, com, lines, subject, organization, university, new, posting\n",
      "-----------------------------\n",
      "Category: rec.autos\n",
      "  Top 10 by average tf-idf\n",
      "    jj, carburetor, tint, kissane, pyramid, schrader, june, wy1z, lojack, howell\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, car, article, writes, subject, organization, lines, cars, just\n",
      "-----------------------------\n",
      "Category: rec.motorcycles\n",
      "  Top 10 by average tf-idf\n",
      "    schultz, kendall, js, dam9543, ron, maine, cervi, teflon, nmm, lusky\n",
      "  Top 10 by total tf-idf\n",
      "    com, edu, bike, article, ca, writes, organization, subject, lines, dod\n",
      "-----------------------------\n",
      "Category: rec.sport.baseball\n",
      "  Top 10 by average tf-idf\n",
      "    comarow, wpi, gidi, rauser, hagins, maine, punjabi, xylogics, reeve, lynch\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, year, article, writes, subject, lines, organization, team, baseball\n",
      "-----------------------------\n",
      "Category: rec.sport.hockey\n",
      "  Top 10 by average tf-idf\n",
      "    ra, howl, twork, ferguson, netlink, cyt, testing, harris, cec2, holger\n",
      "  Top 10 by total tf-idf\n",
      "    edu, ca, game, team, hockey, university, subject, lines, organization, play\n",
      "-----------------------------\n",
      "Category: sci.crypt\n",
      "  Top 10 by average tf-idf\n",
      "    db, jef, rita, agrep, uri, makey, kepley, cadre, ebcdic, pollux\n",
      "  Top 10 by total tf-idf\n",
      "    key, com, edu, clipper, encryption, chip, government, keys, escrow, writes\n",
      "-----------------------------\n",
      "Category: sci.electronics\n",
      "  Top 10 by average tf-idf\n",
      "    jh, mjm, paula, levy, fishman, indiana, tron, plane, rooi, mycal\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, lines, subject, organization, university, use, ca, host, posting\n",
      "-----------------------------\n",
      "Category: sci.med\n",
      "  Top 10 by average tf-idf\n",
      "    gr, clubbing, jl, compartment, hernia, jmetz, spect, cyanamid, kiria, jb\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, pitt, msg, banks, gordon, geb, cs, article, subject\n",
      "-----------------------------\n",
      "Category: sci.space\n",
      "  Top 10 by average tf-idf\n",
      "    wpi, wesleyan, landis, cain, rpi, esoc, punchline, incoming, sics, shread\n",
      "  Top 10 by total tf-idf\n",
      "    edu, space, nasa, com, henry, access, alaska, writes, article, gov\n",
      "-----------------------------\n",
      "Category: soc.religion.christian\n",
      "  Top 10 by average tf-idf\n",
      "    maria, dab, bucknell, halat, oo, dohertyl, storrs, abri, albany, gifford\n",
      "  Top 10 by total tf-idf\n",
      "    god, edu, jesus, people, church, hell, com, christians, subject, believe\n",
      "-----------------------------\n",
      "Category: talk.politics.guns\n",
      "  Top 10 by average tf-idf\n",
      "    sylvain, halat, keele, krueger, nagle, btr, slagle, isc, rit, ama\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, gun, people, guns, writes, don, stratus, article, like\n",
      "-----------------------------\n",
      "Category: talk.politics.mideast\n",
      "  Top 10 by average tf-idf\n",
      "    sy, jle, dg, kt, clamen, convex, kk, misisipi, kelbadjar, pkk\n",
      "  Top 10 by total tf-idf\n",
      "    edu, israel, israeli, jews, com, turkish, people, armenian, armenians, writes\n",
      "-----------------------------\n",
      "Category: talk.politics.misc\n",
      "  Top 10 by average tf-idf\n",
      "    narrative, myers, bram, harris, rolandi, lefty, karadzic, limbaugh, childeren, viability\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, people, writes, article, cramer, optilink, don, government, state\n",
      "-----------------------------\n",
      "Category: talk.religion.misc\n",
      "  Top 10 by average tf-idf\n",
      "    kt, scientology, dm, mayne, oulu, monash, cannibalism, convenient, proteins, emery\n",
      "  Top 10 by total tf-idf\n",
      "    edu, com, god, sandvik, jesus, people, writes, article, christian, organization\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "all_newsgroups = fetch_20newsgroups()\n",
    "all_data = np.array(all_newsgroups.data)\n",
    "for i, category in enumerate(all_newsgroups.target_names):\n",
    "    data = all_data[all_newsgroups.target == i]\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform(data).toarray()\n",
    "    words = vectorizer.get_feature_names()\n",
    "    print \"Category: %s\" % category\n",
    "    avg = np.sum(vectors, axis=0) / np.sum(vectors > 0, axis=0)\n",
    "    print \"  Top 10 by average tf-idf\"\n",
    "    print \"    %s\" % \", \".join(get_top_values(avg, 10, words))\n",
    "    total = np.sum(vectors, axis=0)\n",
    "    print \"  Top 10 by total tf-idf\"\n",
    "    print \"    %s\" % \", \".join(get_top_values(total, 10, words))\n",
    "    print \"-----------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each query, find the three most relevant articles from the 20 newsgroup gorcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/queries.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-638c669137cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# get queries from file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/queries.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/queries.txt'"
     ]
    }
   ],
   "source": [
    "# build vectorizer\n",
    "data = fetch_20newsgroups(subset='train', categories=categories).data\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vectors = vectorizer.fit_transform(data).toarray()\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# get queries from file\n",
    "with open('data/queries.txt') as f:\n",
    "    queries = [line.strip() for line in f]\n",
    "\n",
    "tokenized_queries = vectorizer.transform(queries)\n",
    "cosine_similarities = linear_kernel(tokenized_queries, vectors)\n",
    "titles = newsgroups.filenames\n",
    "for i, query in enumerate(queries):\n",
    "    print query\n",
    "    print get_top_values(cosine_similarities[i], 3, titles)\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
