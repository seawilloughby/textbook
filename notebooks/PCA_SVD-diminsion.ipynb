{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Dimensionality Reduction is exactly what it sounds like. These are techniques for reducing the dimensions.\n",
    "\n",
    "## Why do we want to reduce the dimensions?\n",
    "\n",
    "1. Remove multicolinearity\n",
    "2. Deal with the *curse of dimensionality*\n",
    "3. Remove redundant features\n",
    "4. Interpretation & visualization\n",
    "5. Make computations of algorithms easier\n",
    "6. Discover hidden topics\n",
    "\n",
    "## Standardize your dataset\n",
    "We will always start by standardizing the dataset. This means:\n",
    "\n",
    "1. Center the data for each feature at the mean (so we have mean 0)\n",
    "2. Divide by the standard deviation (so we have std 1)\n",
    "\n",
    "## Covariance Matrix\n",
    "Recall that the covariance matrix is given by:\n",
    "\n",
    "$\\frac{1}{n}M^T M$\n",
    "\n",
    "Note that this is only true because the data is centered around the mean.\n",
    "\n",
    "## Example\n",
    "This is our feature matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    " 10 & 3 \\\\\n",
    " 10 & 4 \\\\\n",
    " 40 & 7 \\\\\n",
    " 60 & 6 \\\\\n",
    " 70 & 9 \\\\\n",
    "100 & 7 \\\\\n",
    "100 & 8\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the feature matrix after we standardize it:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "-1.306 & -1.660 \\\\\n",
    "-1.306 & -1.155 \\\\\n",
    "-0.449 &  0.361 \\\\\n",
    " 0.122 & -0.144 \\\\\n",
    " 0.408 &  1.371 \\\\\n",
    " 1.266 &  0.361 \\\\\n",
    " 1.266 &  0.866\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the covariance matrix:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1.0  & 0.801 \\\\\n",
    "0.801 & 1.0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This tells us that the *covariance* between feature 1 and feature 2 is 0.801. This intuitively makes sense, since we can tell the two features are correlated.\n",
    "\n",
    "The variance of each feature is 1 (this is because we standardized our data first).\n",
    "\n",
    "The computations are done below with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "M:\n",
      "[[-1.30639453 -1.65988202]\n",
      " [-1.30639453 -1.15470054]\n",
      " [-0.44907312  0.36084392]\n",
      " [ 0.12247449 -0.14433757]\n",
      " [ 0.40824829  1.37120689]\n",
      " [ 1.2655697   0.36084392]\n",
      " [ 1.2655697   0.8660254 ]]\n",
      "\n",
      "covariance matrix:\n",
      "[[ 1.          0.80138769]\n",
      " [ 0.80138769  1.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10436ff10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = np.array([[10., 3.], [10., 4.], [40., 7.], [60., 6.], [70., 9.], [100., 7.], [100., 8.]])\n",
    "M = StandardScaler().fit_transform(data)\n",
    "print \"M:\"\n",
    "print M\n",
    "print\n",
    "print \"covariance matrix:\"\n",
    "print 1/7. * M.T.dot(M)\n",
    "plt.scatter(M[:,0], M[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components Analysis (PCA)\n",
    "\n",
    "Usually we will get a covariance matrix with a lot of large values. Our ideal would be one where all the non-diagonal values are 0. This means that there is *no relationship between the features*. We can do a transformation of this data to make this happen!\n",
    "\n",
    "The ideal convariance matrix would look something like this:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "10 & 0 & 0 \\\\\n",
    "0  & 8 & 0 \\\\\n",
    "0  & 0 & 2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "The idea is to find a new set of axis (a *basis*) that better fit the data.\n",
    "\n",
    "We choose the first principal component to be in the direction of the most variance. Here's a look at what we're doing:\n",
    "\n",
    "<img src=\"files/correlated_2d.png\" align=\"left\">\n",
    "<br clear=\"all\">\n",
    "\n",
    "The green line here is the direction with the most variance. This is our first axis.\n",
    "\n",
    "Note that we choose the second dimension (the pink line) to be orthogonal (perpendicular) to the first. There is no covariance between the two features after we rotate the data:\n",
    "\n",
    "<img src=\"files/uncorrelated_2d.png\" align=\"left\">\n",
    "<br clear=\"all\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Mathematically\n",
    "\n",
    "Our goal is to find a transformation matrix $V$ which when applied to $M$ gives us our ideal convariance matrix:\n",
    "\n",
    "$(MV)^T (MV) = V^TM^TMV =\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & \\lambda_3 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & \\lambda_p\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$V$ is the new basis so it should look like this:\n",
    "\n",
    "$V =\n",
    "\\begin{bmatrix}\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid \\\\\n",
    "u_1 & u_2 & u_3 & \\cdots & u_p \\\\\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that the $u_1, u_2, \\ldots, u_p$ is an *orthonormal* basis.\n",
    "\n",
    "*orthonormal* means:\n",
    "1. Vectors are normal (perpendicular) with each other. This means their dot products are 0.\n",
    "2. Vectors have norm 1. This means the dot product with itself is 1.\n",
    "\n",
    "So the following is true:\n",
    "\n",
    "$\\begin{align*}\n",
    "V^TV \n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "- & u_1 & - \\\\\n",
    "- & u_2 & - \\\\\n",
    "- & u_3 & - \\\\\n",
    "- & \\vdots & - \\\\\n",
    "- & u_p & -\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid \\\\\n",
    "u_1 & u_2 & u_3 & \\cdots & u_p \\\\\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "u_1 \\cdot u_1 & u_1 \\cdot u_2 & \\cdots & u_1 \\cdot u_p \\\\\n",
    "u_2 \\cdot u_1 & u_2 \\cdot u_2 & \\cdots & u_2 \\cdot u_p \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "u_p \\cdot u_1 & u_p \\cdot u_2 & \\cdots & u_p \\cdot u_p\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix}\n",
    "\\end{align*}$\n",
    "\n",
    "So $V^TV$ is the identity matrix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the original equation of our ideal world:\n",
    "\n",
    "$\\begin{align*}\n",
    "V^TM^TMV &=\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & \\lambda_3 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & \\lambda_p\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "M^TMV &= V\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & \\lambda_3 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & \\lambda_p\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid \\\\\n",
    "u_1 & u_2 & u_3 & \\cdots & u_p \\\\\n",
    "\\mid & \\mid & \\mid & \\cdots & \\mid\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\lambda_1 & 0 & 0 & \\ldots & 0 \\\\\n",
    "0 & \\lambda_2 & 0 & \\ldots & 0 \\\\\n",
    "0 & 0 & \\lambda_3 & \\ldots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\ldots & \\lambda_p\n",
    "\\end{bmatrix}\n",
    "\\end{align*}$\n",
    "\n",
    "Taking just one of these vectors out, we get:\n",
    "\n",
    "$M^TMu_i = \\lambda_iu_i$\n",
    "\n",
    "So we are looking for the ***eigenvalues*** ($\\lambda_i$) and ***eigenvectors*** ($u_i$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of PCA\n",
    "\n",
    "To get the transformation, we need to find the eigenvalues and eigenvectors of $M^TM$.\n",
    "\n",
    "The eigenvectors are the new basis. The eigenvalues are the variance in each of these dimensions.\n",
    "\n",
    "If we would like to reduce the number of dimensions, we can just get rid of the smallest of the lambdas. To determine how many to keep, we often look at the *scree plot*, a plot of the variances (eigenvalues, also called *loadings*) in increasing order.\n",
    "\n",
    "### Example (MNIST)\n",
    "\n",
    "The MNIST dataset has digits stored as $28 \\times 28$ pixel images. This means there are 784 features. We can use PCA to reduce the dimensions. This is the scree plot:\n",
    "\n",
    "<img src=\"files/screeplot.png\" align=\"left\">\n",
    "<br clear=\"all\">\n",
    "\n",
    "You are generally looking for the elbow in the graph. Here it's around 25 that you stop gaining value from adding more features. You might even be able to get by with just 1 feature!\n",
    "\n",
    "We can get a visual understanding of how much information is kept with each principal component by looking at the visual representation of the first eigenvector. We also here look at the first 10, 50 and 250 eigenvectors.\n",
    "\n",
    "<img src=\"files/mnist_three.png\" align=\"left\">\n",
    "<br clear=\"all\">\n",
    "\n",
    "We also often talk about keeping 80-90% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition (SVD)\n",
    "\n",
    "It's not always easy to directly compute the eigenvalues and eigenvectors. We can use a technique called SVD for more efficient computation. SVD is also useful for discovering hidden topics or ***latent features***.\n",
    "\n",
    "Every matrix has a *unique* decomposition in the following form:\n",
    "\n",
    "$M = U \\Sigma V^T$\n",
    "\n",
    "where\n",
    "* $U$ is column orthogonal: $U^T U = I$\n",
    "* $V$ is column orthogonal: $V^T V = I$\n",
    "* $\\Sigma$ is a diagonal matrix of positive values, where the diagonal is ordered in decreasing order\n",
    "\n",
    "We can reduce the dimensions by sending the smaller of the diagonals to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relationship between PCA and SVD\n",
    "\n",
    "In PCA, we have the following:\n",
    "\n",
    "$M^TMV = V\\Lambda$\n",
    "\n",
    "where $\\Lambda$ is the diagonal matrix of eigenvalues.\n",
    "\n",
    "As $M = U \\Sigma V$ according to SVD:\n",
    "\n",
    "$M^TM = (U \\Sigma V^T)^T U \\Sigma V^T = V \\Sigma^T U^T U \\Sigma V^T = V \\Sigma^2 V^T$\n",
    "\n",
    "So $M^TMV = V \\Sigma^2$.\n",
    "\n",
    "This looks like the same form as PCA, with $\\Lambda = \\Sigma^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD for topic analysis\n",
    "\n",
    "We can use SVD to determine what we call ***latent features***. This will be best demonstrated with an example.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's look at users ratings of different movies. The ratings are from 1-5. A rating of 0 means the user hasn't watched the movie.\n",
    "\n",
    "|       | Matrix | Alien | Serenity | Casablanca | Amelie |\n",
    "| ----- | ------ | ----- | -------- | ---------- | ------ |\n",
    "| **Alice** |      1 |     1 |        1 |          0 |      0 |\n",
    "|   **Bob** |      3 |     3 |        3 |          0 |      0 |\n",
    "| **Cindy** |      4 |     4 |        4 |          0 |      0 |\n",
    "|   **Dan** |      5 |     5 |        5 |          0 |      0 |\n",
    "| **Emily** |      0 |     2 |        0 |          4 |      4 |\n",
    "| **Frank** |      0 |     0 |        0 |          5 |      5 |\n",
    "|  **Greg** |      0 |     1 |        0 |          2 |      2 |\n",
    "\n",
    "Note that the first three movies (Matrix, Alien, Serenity) are Sci-fi movies and the last two (Casablanca, Amelie) are Romance. We will be able to mathematically pull out these topics!\n",
    "\n",
    "Let's do the computation with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0]\n",
      " [3 3 3 0 0]\n",
      " [4 4 4 0 0]\n",
      " [5 5 5 0 0]\n",
      " [0 2 0 4 4]\n",
      " [0 0 0 5 5]\n",
      " [0 1 0 2 2]]\n",
      "=\n",
      "[[-0.14 -0.02 -0.01  0.56 -0.38 -0.7  -0.19]\n",
      " [-0.41 -0.07 -0.03  0.21  0.76 -0.26  0.38]\n",
      " [-0.55 -0.09 -0.04 -0.72 -0.18 -0.34 -0.09]\n",
      " [-0.69 -0.12 -0.05  0.34 -0.23  0.57 -0.12]\n",
      " [-0.15  0.59  0.65  0.    0.2   0.   -0.4 ]\n",
      " [-0.07  0.73 -0.68  0.    0.    0.    0.  ]\n",
      " [-0.08  0.3   0.33  0.   -0.4   0.    0.8 ]]\n",
      "[ 12.48   9.51   1.35   0.     0.  ]\n",
      "[[-0.56 -0.59 -0.56 -0.09 -0.09]\n",
      " [-0.13  0.03 -0.13  0.7   0.7 ]\n",
      " [-0.41  0.8  -0.41 -0.09 -0.09]\n",
      " [-0.71  0.    0.71  0.    0.  ]\n",
      " [ 0.   -0.    0.   -0.71  0.71]]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import svd\n",
    "\n",
    "M = np.array([[1, 1, 1, 0, 0],\n",
    "              [3, 3, 3, 0, 0],\n",
    "              [4, 4, 4, 0, 0],\n",
    "              [5, 5, 5, 0, 0],\n",
    "              [0, 2, 0, 4, 4],\n",
    "              [0, 0, 0, 5, 5],\n",
    "              [0, 1, 0, 2, 2]])\n",
    "\n",
    "u, e, v = svd(M)\n",
    "print M\n",
    "print \"=\"\n",
    "print np.around(u, 2)\n",
    "print np.around(e, 2)\n",
    "print np.around(v, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the results we get:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "3 & 3 & 3 & 0 & 0 \\\\\n",
    "4 & 4 & 4 & 0 & 0 \\\\\n",
    "5 & 5 & 5 & 0 & 0 \\\\\n",
    "0 & 2 & 0 & 4 & 4 \\\\\n",
    "0 & 0 & 0 & 5 & 5 \\\\\n",
    "0 & 1 & 0 & 2 & 2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-0.14 & -0.02 & -0.01 & 0.56 & -0.38 \\\\\n",
    "-0.41 & -0.07 & -0.03 & 0.21 & 0.76 \\\\\n",
    "-0.55 & -0.09 & -0.04 & -0.72 & -0.18 \\\\\n",
    "-0.69 & -0.12 & -0.05 & 0.34 & -0.23 \\\\\n",
    "-0.15 & 0.59 & 0.65 & 0.0 & 0.2 \\\\\n",
    "-0.07 & 0.73 & -0.68 & 0.0 & 0.0 \\\\\n",
    "-0.08 & 0.3 & 0.33 & 0.0 & -0.4\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "12.48 & 0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "0.0 & 9.51 & 0.0 & 0.0 & 0.0 \\\\\n",
    "0.0 & 0.0 & 1.35 & 0.0 & 0.0 \\\\\n",
    "0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\\\\n",
    "0.0 & 0.0 & 0.0 & 0.0 & 0.0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-0.56 & -0.59 & -0.56 & -0.09 & -0.09 \\\\\n",
    "-0.13 & 0.03 & -0.13 & 0.7 & 0.7 \\\\\n",
    "-0.41 & 0.8 & -0.41 & -0.09 & -0.09 \\\\\n",
    "-0.71 & 0.0 & 0.71 & 0.0 & 0.0 \\\\\n",
    "0.0 & -0.0 & 0.0 & -0.71 & 0.71\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that the last two singular values are 0, so we can drop them. Note that these values are 0 because the rank of our original matrix is 3.\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\textbf{1} & \\textbf{1} & \\textbf{1} & 0 & 0 \\\\\n",
    "\\textbf{3} & \\textbf{3} & \\textbf{3} & 0 & 0 \\\\\n",
    "\\textbf{4} & \\textbf{4} & \\textbf{4} & 0 & 0 \\\\\n",
    "\\textbf{5} & \\textbf{5} & \\textbf{5} & 0 & 0 \\\\\n",
    "0 & \\textbf{2} & 0 & \\textbf{4} & \\textbf{4} \\\\\n",
    "0 & 0 & 0 & \\textbf{5} & \\textbf{5} \\\\\n",
    "0 & \\textbf{1} & 0 & \\textbf{2} & \\textbf{2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "-0.14 & -0.02 & -0.01 \\\\\n",
    "\\textbf{-0.41} & -0.07 & -0.03 \\\\\n",
    "\\textbf{-0.55} & -0.09 & -0.04 \\\\\n",
    "\\textbf{-0.69} & -0.12 & -0.05 \\\\\n",
    "-0.15 & \\textbf{0.59} & \\textbf{0.65} \\\\\n",
    "-0.07 & \\textbf{0.73} & \\textbf{-0.68} \\\\\n",
    "-0.08 & \\textbf{0.3} & \\textbf{0.33}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\textbf{12.48} & 0.0 & 0.0 \\\\\n",
    "0.0 & \\textbf{9.51} & 0.0 \\\\\n",
    "0.0 & 0.0 & \\textbf{1.35}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\textbf{-0.56} & \\textbf{-0.59} & \\textbf{-0.56} & -0.09 & -0.09 \\\\\n",
    "-0.13 & 0.03 & -0.13 & \\textbf{0.7} & \\textbf{0.7} \\\\\n",
    "\\textbf{-0.41} & \\textbf{0.8} & \\textbf{-0.41} & -0.09 & -0.09\n",
    "\\end{bmatrix}$\n",
    "\n",
    "You can see the two topics fall out:\n",
    "\n",
    "1. Science Fiction\n",
    "    * First singular value (12.4)\n",
    "    * First column of the $U$ matrix (note that the first four users have large values here)\n",
    "    * First row of the $V$ matrix (note that the first three movies have large values here)\n",
    "2. Romance\n",
    "    * Second singular value (9.5)\n",
    "    * Second column of the $U$ matrix (note that the last three users have large values here)\n",
    "    * Second row of the $V$ matrix (note that the last two movies have large values here)\n",
    "\n",
    "$U$ is the ***user-to-topic*** matrix and $V$ is the ***movie-to-topic*** matrix.\n",
    "\n",
    "The third singular value is relatively small, so we can exclude it with little loss of data. Let's try doing that and reconstruct our matrix!\n",
    "\n",
    "$\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "3 & 3 & 3 & 0 & 0 \\\\\n",
    "4 & 4 & 4 & 0 & 0 \\\\\n",
    "5 & 5 & 5 & 0 & 0 \\\\\n",
    "0 & 2 & 0 & 4 & 4 \\\\\n",
    "0 & 0 & 0 & 5 & 5 \\\\\n",
    "0 & 1 & 0 & 2 & 2\n",
    "\\end{bmatrix}\n",
    "&\\approx\n",
    "\\begin{bmatrix}\n",
    "-0.14 & -0.02 \\\\\n",
    "-0.41 & -0.07 \\\\\n",
    "-0.55 & -0.09 \\\\\n",
    "-0.69 & -0.12 \\\\\n",
    "-0.15 & 0.59 \\\\\n",
    "-0.07 & 0.73 \\\\\n",
    "-0.08 & 0.3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "12.48 & 0.0 \\\\\n",
    "0.0 & 9.51\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "-0.56 & -0.59 & -0.56 & -0.09 & -0.09 \\\\\n",
    "-0.13 & 0.03 & -0.13 & 0.7 & 0.7\n",
    "\\end{bmatrix} \\\\\n",
    "&=\n",
    "\\begin{bmatrix}\n",
    "0.99 & 1.01 & 0.99 & -0.0 & -0.0 \\\\\n",
    "2.98 & 3.04 & 2.98 & -0.0 & -0.0 \\\\\n",
    "3.98 & 4.05 & 3.98 & -0.01 & -0.01 \\\\\n",
    "4.97 & 5.06 & 4.97 & -0.01 & -0.01 \\\\\n",
    "0.36 & 1.29 & 0.36 & 4.08 & 4.08 \\\\\n",
    "-0.37 & 0.73 & -0.37 & 4.92 & 4.92 \\\\\n",
    "0.18 & 0.65 & 0.18 & 2.04 & 2.04\n",
    "\\end{bmatrix}\n",
    "\\end{align*}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}