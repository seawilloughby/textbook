{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Bagging\n",
    " - Bagging, also known as bootstrap aggregating, is for running multiple models in parallel (the models don't use each other's results in order to predict). Each model gets a vote on the final prediction.\n",
    "\n",
    " - For classification problems (predicting a categorical value), we choose the label with the most votes.\n",
    "\n",
    " - For regression problems (predicting a continuous value), we average the values given by all the models.\n",
    "\n",
    " - You can bag with any collection of algorithms, giving them each a vote to the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#Random Forest\n",
    "The idea is to repeatedly randomly select data from the dataset (with replacement) and build a Decision Tree with each new sample. The default is to have the randomly selected data be the same size as the initial dataset. Note that since we are sampling with replacement, many data points will be repeated in the sample and many won't be included.\n",
    "\n",
    "Random Forests also limit each node of the Decision Tree to only consider splitting on a random subset of the features.\n",
    "\n",
    "##Out of Bag Error \n",
    "We can analyze a Random Forest using the standard cross validation method of splitting the dataset into a training set and a testing set. However, if we're clever, we notice that each tree doesn't see all of the training data, so we can use the skipped data to cross validate each tree individually.\n",
    "\n",
    "when selecting from the dataset, about one third of the data is left out (discussed here if you want to think about the math). So every data point can be tested with about 1/3 of the trees. We calculate the percent of these that we get correct, and this is the out-of-bag error.\n",
    "\n",
    "It has been proven that this is sufficient and that cross validation is not strictly necessary for a random forest, but we often still use it as that makes it easier to compare with other models.\n",
    "\n",
    "##Feature Importance:\n",
    "ten still use it as that makes it easier to compare with other models.\n",
    "\n",
    "Breiman, the originator of random forests, uses out-of-bag error to determine feature importance, discussed here. The idea is to compare the out-of-bag error of the trees with the out-of-bag error of the trees if you change the feature's value (basically, if we screw with the value of the feature, how much does that impact the total error?).\n",
    "```\n",
    "For every tree:\n",
    "        Take the data that is not covered by the tree.Randomly permute the values of the feature (i.e. keep the same values, but shuffle them around the data points).\n",
    "    Calculate the OOB error on the data with the feature values permuted.\n",
    "    Subtract the permutated OOB from the OOB of the original data to get the feature importance on this tree.\n",
    "    Average all the individual feature importances to get the feature importance.\n",
    "```\n",
    "\n",
    "Sklearn:\n",
    "\n",
    "Their method doesn't involve using the out-of-bag score. Basically, the higher in the tree the feature is, the more important it is in determining the result of a data point. The expected fraction of data points that reach a node is used as an estimate of that feature's importance for that tree. Then average those values across all trees to get the feature's importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####example instantiations of algorithms  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import all the things from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clwilloughby/anaconda/lib/python2.7/site-packages/numpy/core/fromnumeric.py:2507: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.\n",
      "  VisibleDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a test dataset for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boston = load_boston() \n",
    "y = boston.target #house prices\n",
    "x = boston.data #13 features\n",
    "\n",
    "#put it into a test train set\n",
    "x_train, x_test, y_train, y_test =  train_test_split(x, y, test_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Instantiating Estimators:\n",
    "\n",
    "####RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c1e75f92d875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m rf = RandomForestRegressor(n_estimators=100,\n\u001b[0m\u001b[1;32m      2\u001b[0m                            \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                            random_state=1)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m random_forest_grid = {'max_depth': [3, None],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100,\n",
    "                           n_jobs=-1,\n",
    "                           random_state=1)\n",
    "\n",
    "#from class notes:\n",
    "random_forest_grid = {'max_depth': [3, None],\n",
    "                      'max_features': ['sqrt', 'log2', None],\n",
    "                      'min_samples_split': [1, 2, 4],\n",
    "                      'min_samples_leaf': [1, 2, 4],\n",
    "                      'bootstrap': [True, False],\n",
    "                      'n_estimators': [10, 20, 40],\n",
    "                      'random_state': [1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(learning_rate=0.1,\n",
    "                                 loss='ls',\n",
    "                                 n_estimators=100,\n",
    "                                 random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####max_depth:\n",
    "1. controls depth of interaction\n",
    "2. so, how many branches the tree goes. May be a stump of 1\n",
    "3. normally no more than 4, 6.\n",
    "\n",
    "#####min_samples_per_leaf\n",
    "1. don't want too few leaves (like only 1) , because then overfit to outliers\n",
    "\n",
    "#####n_estimators\n",
    "1. number of trees grown\n",
    "\n",
    "#####learning_rate\n",
    "1. slow is good. Lower rate needs more estimators \n",
    "2. really important parameter to tune!\n",
    "\n",
    "###Stochastic Gradient Boosting\n",
    "both of these can improve accuracy and reduce runtime\n",
    "#####max_features\n",
    "1. good when lots of features- randomly sample them\n",
    "\n",
    "####sub_sample\n",
    "1. random subset of training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abr = AdaBoostRegressor(DecisionTreeRegressor(),\n",
    "                        learning_rate=0.1,\n",
    "                        loss='linear',\n",
    "                        n_estimators=100,\n",
    "                        random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at every tree iteration,\n",
    "anything that was wrong (Residual) is given a weight for the next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Run GradientBoosting in SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "#make a dictinoary of the parameters you want to alter in the gridsearch\n",
    "param_grid = {'learning rate': [0.1 0.05, 0.01] \n",
    "             'max depth': [4,6]\n",
    "             'min_samples_leaf': [3,4,5,9,17]\n",
    "             'max_features': [1,3,8]}\n",
    "\n",
    "\n",
    "\n",
    "'''instantiate your estimator. So, \n",
    "    AdaBoostRegressor\n",
    "    GradientBoostingRegressor\n",
    "    RandomForestRegressor'''\n",
    "#if changing learning rate, maybe make n estimators even higher...\n",
    "est = GradientBoostingRegressor(n_estimators = 3000)\n",
    "\n",
    "#run through ALL the parameters, and fit the data to all permutations\n",
    "gs_cv = GridSearchCV(est, param_grid).fit(x_train,y_train)\n",
    "#this will tell you what the best permaeters were\n",
    "gs_cv.best_params\n",
    "\n",
    "#if you want to call the chosen model, can also use this:\n",
    "best_rf_model = rf_gridsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#another example\n",
    "rf_gridsearch = GridSearchCV(RandomForestRegressor(),\n",
    "                             random_forest_grid,\n",
    "                             n_jobs=-1,\n",
    "                             verbose=True,\n",
    "                             scoring='mean_squared_error')\n",
    "\n",
    "rf_gridsearch.fit(train_x, train_y)\n",
    "\n",
    "print \"best parameters:\", gridcv.best_params_\n",
    "\n",
    "best_rf_model = rf_gridsearch.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
