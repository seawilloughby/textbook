{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import cross_validation, linear_model, datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Precison = TP / (TP + FP) = TP / (predicted yes)\n",
    "1. Recall = TP / (TP + FN) = TP / (actual yes) \n",
    "1. F1 = 2 / (1/Precision + 1/Recall) = 2 * Precision * Recall / (Precision + Recall) = 2TP / (2TP + FN + FP) \n",
    "1. Accuracy = (TP + TN) / (TP + FP + TN + FN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53292886980622434"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_information_gain(S, v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression: Minimize RSS\n",
    "1. Ridge Regression: includes a tuning parameter, penalize large Betas\n",
    "1. Lasso Regression: Allows the penaltay on large coeffecients to go to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#answer from an assessment\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "def linear_regression(X_train, y_train, X_test, y_test):\n",
    "    ''' \n",
    "    INPUT: 2 DIMENSIONAL NUMPY ARRAY, NUMPY ARRAY\n",
    "    OUTPUT: TUPLE OF FLOATS, FLOAT\n",
    "    Use the sklearn LinearRegression to find the best fit line for X_train and\n",
    "    y_train. Calculate the R^2 value for X_test and y_test.\n",
    "    Return a tuple of the coeffients and the R^2 value. Should be in this form:\n",
    "    (12.3, 9.5), 0.567\n",
    "    '''\n",
    "    regr = LinearRegression()\n",
    "    regr.fit(X_train, y_train)\n",
    "    return regr.coef_, regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "X_data = preprocessing.scale(X)\n",
    "fit = Ridge(alpha=a, normalize=True).fit(X_data, y)\n",
    "params[i] = fit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RidgeCV  #automatically tunes the alpha, or shrinkage of coeffeicents by crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "X_data = preprocessing.scale(X)\n",
    "fit = linear_model.Lasso(alpha=a, normalize=True).fit(X_data, y)\n",
    "params[i] = fit.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i did my own regression.. how do I check residuals?,,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#QQ plot! compare theoretical quantiles to sample quantiles\n",
    "#also look at VIF\n",
    "statsmodels.graphics.gofplots.qqplot()\n",
    "#get outliers\n",
    "statsmodels.graphics.regression. influence_plot()\n",
    "#multicolinarity. Variance Inflation Factor. VIF greater than 10\n",
    "#means the feature is collinear with at least another\n",
    "'''variance_inflation_factor takes a matrix of features (numpy matrix, not pandas dataframe) and the column index of the feature the VIF is to be calculated\n",
    "Write a function that loops through and calculates the VIF for each of the features'''\n",
    "statsmodels.stats.outliers_influence.variance_inflation_Factor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put categorical data into dummy variables \n",
    "http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'versicolor'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.get_dummies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run my function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def drxt_xvalidation(X, y, model_type, num_k=5, ):\n",
    "    '''Uses skykit learn to do cross validation with your specified number of folds and model type for the data.\n",
    "    X and y are numpy arrays containing the features, and predicted values respectively.\n",
    "    model_type is a '''\n",
    "    \n",
    "    #tell kf the length of your data set, and how many folds you want\n",
    "    kf = cross_validation.KFold(len(y), n_folds=num_k, shuffle= True)\n",
    "    \n",
    "    for train_index, test_index in kf:\n",
    "        #iterate through the training and test index to shuffle your set \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        #now create a model for each k fold and test them! \n",
    "        coef_list = []  #what coefficeients was it?! \n",
    "        score_list = [] #how good was the model?! \n",
    "        \n",
    "        for train_index, test_index in kf:\n",
    "            #make a new linear model for each kfold\n",
    "            mod = model_type()\n",
    "\n",
    "            #extract your train and test set\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            #now fit the model\n",
    "            mod.fit(X_train, y_train)\n",
    "\n",
    "            #get the coeffecients and add them for later! \n",
    "            coef_list.append(mod.coef_)\n",
    "\n",
    "            #get a fun score to judge them!\n",
    "            score_list.append(mod.score(X_test, y_test))\n",
    "            \n",
    "\n",
    "    return coef_list, score_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
